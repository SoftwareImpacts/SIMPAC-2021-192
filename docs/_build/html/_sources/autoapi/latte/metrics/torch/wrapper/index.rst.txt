:py:mod:`latte.metrics.torch.wrapper`
=====================================

.. py:module:: latte.metrics.torch.wrapper

.. autoapi-nested-parse::

   
   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   latte.metrics.torch.wrapper.TorchMetricsFunctionalWrapper
   latte.metrics.torch.wrapper.TorchMetricsModuleWrapper



Functions
~~~~~~~~~

.. autoapisummary::

   latte.metrics.torch.wrapper.to_numpy



.. py:function:: to_numpy(args, kwargs)

   
















   ..
       !! processed by numpydoc !!

.. py:class:: TorchMetricsFunctionalWrapper(metric: Callable, name: Optional[str] = None, dist_sync_on_step: bool = False, process_group: Optional[Any] = None, dist_sync_fn: Callable = None, **kwargs)

   Bases: :py:obj:`torchmetrics.Metric`

   
   Base class for all metrics present in the Metrics API.

   Implements ``add_state()``, ``forward()``, ``reset()`` and a few other things to
   handle distributed synchronization and per-step metric computation.

   Override ``update()`` and ``compute()`` functions to implement your own metric. Use
   ``add_state()`` to register metric state variables which keep track of state on each
   call of ``update()`` and are synchronized across processes when ``compute()`` is called.

   .. note::

      Metric state variables can either be ``torch.Tensors`` or an empty list which can we used
      to store `torch.Tensors``.

   .. note::

      Different metrics only override ``update()`` and not ``forward()``. A call to ``update()``
      is valid, but it won't return the metric value at the current step. A call to ``forward()``
      automatically calls ``update()`` and also returns the metric value at the current step.

   :param compute_on_step: Forward only calls ``update()`` and returns None if this is set to False. default: True
   :param dist_sync_on_step: Synchronize metric state across processes at each ``forward()``
                             before returning the value at the step.
   :param process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world)
   :param dist_sync_fn: Callback that performs the allgather operation on the metric state. When `None`, DDP
                        will be used to perform the allgather.















   ..
       !! processed by numpydoc !!
   .. py:method:: update(self, *args, **kwargs)

      
      Override this method to update the state variables of your metric class.
















      ..
          !! processed by numpydoc !!

   .. py:method:: compute(self, *args, **kwargs)

      
      Override this method to compute the final metric value from state variables synchronized across the
      distributed backend.
















      ..
          !! processed by numpydoc !!


.. py:class:: TorchMetricsModuleWrapper(metric: Callable, compute_on_step: bool, dist_sync_on_step: bool = False, process_group: Optional[Any] = None, dist_sync_fn: Callable = None)

   Bases: :py:obj:`torchmetrics.Metric`

   
   Base class for all metrics present in the Metrics API.

   Implements ``add_state()``, ``forward()``, ``reset()`` and a few other things to
   handle distributed synchronization and per-step metric computation.

   Override ``update()`` and ``compute()`` functions to implement your own metric. Use
   ``add_state()`` to register metric state variables which keep track of state on each
   call of ``update()`` and are synchronized across processes when ``compute()`` is called.

   .. note::

      Metric state variables can either be ``torch.Tensors`` or an empty list which can we used
      to store `torch.Tensors``.

   .. note::

      Different metrics only override ``update()`` and not ``forward()``. A call to ``update()``
      is valid, but it won't return the metric value at the current step. A call to ``forward()``
      automatically calls ``update()`` and also returns the metric value at the current step.

   :param compute_on_step: Forward only calls ``update()`` and returns None if this is set to False. default: True
   :param dist_sync_on_step: Synchronize metric state across processes at each ``forward()``
                             before returning the value at the step.
   :param process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world)
   :param dist_sync_fn: Callback that performs the allgather operation on the metric state. When `None`, DDP
                        will be used to perform the allgather.















   ..
       !! processed by numpydoc !!
   .. py:method:: update(self, *args, **kwargs)
      :abstractmethod:

      
      Override this method to update the state variables of your metric class.
















      ..
          !! processed by numpydoc !!

   .. py:method:: compute(self)
      :abstractmethod:

      
      Override this method to compute the final metric value from state variables synchronized across the
      distributed backend.
















      ..
          !! processed by numpydoc !!


